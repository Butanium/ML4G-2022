{"cells":[{"cell_type":"markdown","metadata":{"id":"2XnVcCECKzHU"},"source":["## Vanilla Policy Optimisation\n","\n","<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d5/vanilla_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","\n","Preliminary questions:\n","- Run the script with the defaults parameters on the terminal\n","- Explain from torch.distributions.categorical import Categorical\n","- google gym python, why is it useful?\n","- Policy gradient is model based or model free?\n","- Is policy gradient on-policy or off-policy?\n","\n","Read all the code, then:\n","- Use https://github.com/patrick-kidger/torchtyping to type the functions get_policy, get_action and compute_loss\n","- Type completely the whole code.\n","- Use from typeguard import typechecked and the @typechecked decorator to check the previous question.\n","- Answer the questions\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"gZy82ZTkKzHb","executionInfo":{"status":"ok","timestamp":1667325903207,"user_tz":-60,"elapsed":285,"user":{"displayName":"Clément Dumas (Butanium)","userId":"16874386063895644463"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.distributions.categorical import Categorical\n","from torch.optim import Adam\n","import numpy as np\n","import gym\n","from gym.spaces import Discrete, Box\n","from torchtyping import TensorType, patch_typeguard\n","from typeguard import typechecked\n","\n","patch_typeguard()  # use before @typechecked\n","\n","def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n","    # Build a feedforward neural network.\n","    layers = []\n","    for j in range(len(sizes)-1):\n","        act = activation if j < len(sizes)-2 else output_activation\n","        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n","    # What does * mean here? Search for unpacking in python\n","    # We unpack the tuple\n","    return nn.Sequential(*layers)\n","\n","def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n","          epochs=50, batch_size=5000, render=False):\n","\n","    # make environment, check spaces, get obs / act dims\n","    env = gym.make(env_name)\n","    assert isinstance(env.observation_space, Box), \\\n","        \"This example only works for envs with continuous state spaces.\"\n","    assert isinstance(env.action_space, Discrete), \\\n","        \"This example only works for envs with discrete action spaces.\"\n","\n","    obs_dim = env.observation_space.shape[0]\n","    n_acts = env.action_space.n\n","\n","    # Core of policy network\n","    # What should be the sizes of the layers of the policy network?\n","    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n","    # obs_dim -> ... -> A\n","\n","\n","    # make function to compute action distribution\n","    # What is the shape of obs?\n","    @typechecked\n","    def get_policy(obs : TensorType[..., obs_dim]) -> Categorical:\n","        # Warning: obs has not always the same shape.\n","        logits = logits_net(obs)\n","        return Categorical(logits=logits)\n","\n","    # make action selection function (outputs int actions, sampled from policy)\n","    # What is the shape of obs?\n","    @typechecked\n","    def get_action(obs : TensorType[..., obs_dim]) -> int:\n","        return get_policy(obs).sample().item()\n","\n","    # make loss function whose gradient, for the right data, is policy gradient\n","    # What does the weights parameter represents here?\n","    # What is the shape of obs?\n","    @typechecked\n","    def compute_loss(obs : TensorType[..., obs_dim], act : TensorType[...], weights : TensorType[...]) \\\n","      -> TensorType[()]:\n","        logp = get_policy(obs).log_prob(act)\n","        return -(logp * weights).mean()\n","\n","    # make optimizer\n","    optimizer = Adam(logits_net.parameters(), lr=lr)\n","\n","    # for training policy\n","    @typechecked\n","    def train_one_epoch() -> (TensorType[()], TensorType[...], TensorType[...]):\n","        # make some empty lists for logging.\n","        batch_obs = []          # for observations\n","        batch_acts = []         # for actions\n","        batch_weights = []      # for R(tau) weighting in policy gradient\n","        batch_rets = []         # for measuring episode returns # What is the return?\n","        batch_lens = []         # for measuring episode lengths\n","\n","        # reset episode-specific variables\n","        obs = env.reset()       # first obs comes from starting distribution \n","        done = False            # signal from environment that episode is over\n","        ep_rews = []            # list for rewards accrued throughout ep\n","\n","        # render first episode of each epoch\n","        finished_rendering_this_epoch = False\n","\n","        # collect experience by acting in the environment with current policy\n","        while True:\n","\n","            # rendering\n","            if (not finished_rendering_this_epoch) and render:\n","                env.render()\n","\n","            # save obs\n","            batch_obs.append(obs.copy())\n","\n","            # act in the environment\n","            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n","            obs, rew, done, _ = env.step(act)\n","\n","            # save action, reward\n","            batch_acts.append(act)\n","            ep_rews.append(rew)\n","\n","            if done:\n","                # if episode is over, record info about episode\n","                # Is the reward discounted?\n","                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n","                batch_rets.append(ep_ret)\n","                batch_lens.append(ep_len)\n","\n","                # the weight for each logprob(a|s) is R(tau)\n","                # Why do we use a constant vector here?\n","                batch_weights += [ep_ret] * ep_len\n","\n","                # reset episode-specific variables\n","                obs, done, ep_rews = env.reset(), False, []\n","\n","                # won't render again this epoch\n","                finished_rendering_this_epoch = True\n","\n","                # end experience loop if we have enough of it\n","                if len(batch_obs) > batch_size:\n","                    break\n","\n","        # take a single policy gradient update step\n","        optimizer.zero_grad()\n","        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n","                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n","                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n","                                  )\n","        batch_loss.backward()\n","        optimizer.step()\n","        return batch_loss, batch_rets, batch_lens\n","\n","    # training loop\n","    for i in range(epochs):\n","        batch_loss, batch_rets, batch_lens = train_one_epoch()\n","        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n","                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"JaL-0QxZKzHf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667326333635,"user_tz":-60,"elapsed":2415,"user":{"displayName":"Clément Dumas (Butanium)","userId":"16874386063895644463"}},"outputId":"6a363049-f7a6-4748-f45b-5d77b9bc500e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  f\"The environment {id} is out of date. You should consider \"\n","/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"]},{"output_type":"stream","name":"stdout","text":["epoch:   0 \t loss: 19.801 \t return: 26.000 \t ep_len: 26.000\n","epoch:   1 \t loss: 19.628 \t return: 28.000 \t ep_len: 28.000\n","epoch:   2 \t loss: 24.686 \t return: 32.000 \t ep_len: 32.000\n","epoch:   3 \t loss: 28.666 \t return: 27.667 \t ep_len: 27.667\n","epoch:   4 \t loss: 10.457 \t return: 15.500 \t ep_len: 15.500\n","epoch:   5 \t loss: 14.449 \t return: 16.750 \t ep_len: 16.750\n","epoch:   6 \t loss: 27.305 \t return: 33.000 \t ep_len: 33.000\n","epoch:   7 \t loss: 9.304 \t return: 14.250 \t ep_len: 14.250\n","epoch:   8 \t loss: 22.223 \t return: 32.500 \t ep_len: 32.500\n","epoch:   9 \t loss: 18.798 \t return: 22.667 \t ep_len: 22.667\n","epoch:  10 \t loss: 17.482 \t return: 20.500 \t ep_len: 20.500\n","epoch:  11 \t loss: 10.588 \t return: 14.750 \t ep_len: 14.750\n","epoch:  12 \t loss: 12.180 \t return: 18.333 \t ep_len: 18.333\n","epoch:  13 \t loss: 11.815 \t return: 16.250 \t ep_len: 16.250\n","epoch:  14 \t loss: 24.219 \t return: 27.500 \t ep_len: 27.500\n","epoch:  15 \t loss: 12.327 \t return: 16.750 \t ep_len: 16.750\n","epoch:  16 \t loss: 8.695 \t return: 13.000 \t ep_len: 13.000\n","epoch:  17 \t loss: 17.682 \t return: 21.333 \t ep_len: 21.333\n","epoch:  18 \t loss: 10.295 \t return: 16.000 \t ep_len: 16.000\n","epoch:  19 \t loss: 14.589 \t return: 15.500 \t ep_len: 15.500\n","epoch:  20 \t loss: 27.369 \t return: 27.333 \t ep_len: 27.333\n","epoch:  21 \t loss: 14.804 \t return: 19.667 \t ep_len: 19.667\n","epoch:  22 \t loss: 13.670 \t return: 18.000 \t ep_len: 18.000\n","epoch:  23 \t loss: 11.073 \t return: 15.250 \t ep_len: 15.250\n","epoch:  24 \t loss: 11.389 \t return: 16.000 \t ep_len: 16.000\n","epoch:  25 \t loss: 10.831 \t return: 15.500 \t ep_len: 15.500\n","epoch:  26 \t loss: 18.329 \t return: 20.000 \t ep_len: 20.000\n","epoch:  27 \t loss: 13.969 \t return: 18.333 \t ep_len: 18.333\n","epoch:  28 \t loss: 25.097 \t return: 31.000 \t ep_len: 31.000\n","epoch:  29 \t loss: 20.533 \t return: 22.667 \t ep_len: 22.667\n","epoch:  30 \t loss: 14.034 \t return: 17.333 \t ep_len: 17.333\n","epoch:  31 \t loss: 13.389 \t return: 18.333 \t ep_len: 18.333\n","epoch:  32 \t loss: 14.436 \t return: 20.333 \t ep_len: 20.333\n","epoch:  33 \t loss: 15.075 \t return: 17.333 \t ep_len: 17.333\n","epoch:  34 \t loss: 13.154 \t return: 18.333 \t ep_len: 18.333\n","epoch:  35 \t loss: 17.456 \t return: 23.667 \t ep_len: 23.667\n","epoch:  36 \t loss: 29.853 \t return: 31.500 \t ep_len: 31.500\n","epoch:  37 \t loss: 12.269 \t return: 16.250 \t ep_len: 16.250\n","epoch:  38 \t loss: 15.553 \t return: 20.000 \t ep_len: 20.000\n","epoch:  39 \t loss: 13.003 \t return: 18.000 \t ep_len: 18.000\n","epoch:  40 \t loss: 19.971 \t return: 28.500 \t ep_len: 28.500\n","epoch:  41 \t loss: 21.673 \t return: 28.000 \t ep_len: 28.000\n","epoch:  42 \t loss: 21.535 \t return: 29.000 \t ep_len: 29.000\n","epoch:  43 \t loss: 39.657 \t return: 37.667 \t ep_len: 37.667\n","epoch:  44 \t loss: 10.839 \t return: 15.500 \t ep_len: 15.500\n","epoch:  45 \t loss: 22.894 \t return: 29.500 \t ep_len: 29.500\n","epoch:  46 \t loss: 10.204 \t return: 13.750 \t ep_len: 13.750\n","epoch:  47 \t loss: 18.418 \t return: 26.000 \t ep_len: 26.000\n","epoch:  48 \t loss: 21.331 \t return: 28.000 \t ep_len: 28.000\n","epoch:  49 \t loss: 19.317 \t return: 26.000 \t ep_len: 26.000\n"]}],"source":["train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n","          epochs=50, batch_size=50, render=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmdS-eHuKzHg"},"outputs":[],"source":["# Original algo here: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/vpg/vpg.py"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"https://github.com/EffiSciencesResearch/ML4G/blob/main/days/w1d5/vanilla_policy_gradient.ipynb","timestamp":1667322585676}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}